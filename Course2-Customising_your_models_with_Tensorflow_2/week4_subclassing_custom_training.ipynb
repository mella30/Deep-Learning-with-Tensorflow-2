{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Coding Tutorial.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mella30/Deep-Learning-with-Tensorflow-2/blob/main/Course2-Customising_your_models_with_Tensorflow_2/week4_subclassing_custom_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_Vdv9xfLUT9",
        "outputId": "8fa7ec27-f1da-4955-b7ad-8cfea27c7d81"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODTr_t4VLUUB"
      },
      "source": [
        "# Model subclassing and custom training loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpLf8goaLUUC"
      },
      "source": [
        " ## Coding tutorials\n",
        " #### [1. Model subclassing](#coding_tutorial_1)\n",
        " #### [2. Custom layers](#coding_tutorial_2)\n",
        " #### [3. Automatic differentiation](#coding_tutorial_3)\n",
        " #### [4. Custom training loops](#coding_tutorial_4)\n",
        " #### [5. tf.function decorator](#coding_tutorial_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkBUlKjaLUUE"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_1\"></a>\n",
        "## Model subclassing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9Btd3AmLUUF"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Softmax, concatenate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufaa6AMZLUUF"
      },
      "source": [
        "#### Create a simple model using the model subclassing API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmKcYHEoLUUG"
      },
      "source": [
        "# Build the model\n",
        "# One branch has only the dense one layer, the other has the dense two, a dense three layers sequentially.\n",
        "# Then the outputs of both branches can be concatenated by just writing concatenate. \n",
        "\n",
        "class MyModel(Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "\n",
        "    self.dense_1 = Dense(64, activation='relu')\n",
        "    self.dense_2 = Dense(10)\n",
        "    self.dense_3 = Dense(5)\n",
        "    self.softmax = Softmax()\n",
        "\n",
        "  def call(self, inputs, training=True):\n",
        "    x = self.dense_1(inputs)\n",
        "    y1 = self.dense_2(inputs)\n",
        "    y2 = self.dense_3(y1)\n",
        "    concat = concatenate([x,y2])\n",
        "    return self.softmax(concat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLJ57PmfLUUG",
        "outputId": "888ed13a-261c-4d82-fe35-6f21cb1862c9"
      },
      "source": [
        "# Print the model summary\n",
        "\n",
        "model = MyModel()\n",
        "model(tf.random.uniform([1,10]))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                multiple                  704       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  110       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  55        \n",
            "_________________________________________________________________\n",
            "softmax (Softmax)            multiple                  0         \n",
            "=================================================================\n",
            "Total params: 869\n",
            "Trainable params: 869\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8w1peLbLUUH"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_2\"></a>\n",
        "## Custom layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5jndnizLUUI"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, Softmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aSbZBB7LUUJ"
      },
      "source": [
        "#### Create custom layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBPsloJ5LUUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa9eadf7-5100-44b4-e78a-23ba4160e0ca"
      },
      "source": [
        "# Create a custom layer\n",
        "\n",
        "class MyLayer(Layer):\n",
        "\n",
        "  def __init__(self, units, input_dim):\n",
        "    super(MyLayer, self).__init__()\n",
        "    self.w = self.add_weight(shape=(input_dim, units),\n",
        "                             initializer='random_normal')\n",
        "    self.b = self.add_weight(shape=(units,),\n",
        "                             initializer='zeros')\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    return tf.matmul(inputs, self.w)+self.b\n",
        "\n",
        "dense_layer = MyLayer(3,5)\n",
        "x = tf.ones((1,5))\n",
        "print(dense_layer(x))\n",
        "print(dense_layer.weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[ 0.03809061  0.15602739 -0.11207093]], shape=(1, 3), dtype=float32)\n",
            "[<tf.Variable 'Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
            "array([[-0.11872357,  0.04890529, -0.02663227],\n",
            "       [ 0.03091317,  0.02336162, -0.08939604],\n",
            "       [ 0.08055347,  0.11619683, -0.02668897],\n",
            "       [ 0.05359671, -0.02418938,  0.03706935],\n",
            "       [-0.00824917, -0.00824697, -0.006423  ]], dtype=float32)>, <tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2tENUI8LUUL"
      },
      "source": [
        "# Specify trainable weights (to freeze parts of the layers weights) \n",
        "\n",
        "class MyLayerNontrainable(Layer):\n",
        "\n",
        "  def __init__(self, units, input_dim):\n",
        "    super(MyLayerNontrainable, self).__init__()\n",
        "    self.w = self.add_weight(shape=(input_dim, units),\n",
        "                             initializer='random_normal',\n",
        "                             trainable=False)\n",
        "    self.b = self.add_weight(shape=(units,),\n",
        "                             initializer='zeros',\n",
        "                             trainable=False)\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    return tf.matmul(inputs, self.w)+self.b\n",
        "\n",
        "dense_layer_nontrainable = MyLayerNontrainable(3,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GGVI6adLUUL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca57e0a2-e62d-4e08-d09f-18a2b9928ac5"
      },
      "source": [
        "print('trainable weights:', len(dense_layer_nontrainable.trainable_weights))\n",
        "print('non-trainable weights:', len(dense_layer_nontrainable.non_trainable_weights))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trainable weights: 0\n",
            "non-trainable weights: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iw7rdoRLUUM"
      },
      "source": [
        "# Create a custom layer to accumulate means of output values\n",
        "# \n",
        "\n",
        "class MyLayerMean(Layer):\n",
        "\n",
        "  def __init__(self, units, input_dim):\n",
        "    super(MyLayerMean, self).__init__()\n",
        "    self.w = self.add_weight(shape=(input_dim, units),\n",
        "                             initializer='random_normal')\n",
        "    self.b = self.add_weight(shape=(units,),\n",
        "                             initializer='zeros')\n",
        "    # accumulate means of output values everytime it is called\n",
        "    self.sum_activation = tf.Variable(initial_value=tf.zeros((units,)),\n",
        "                                      trainable=False)\n",
        "    # counts the number of times the layer has been called\n",
        "    self.number_call = tf.Variable(initial_value=0,\n",
        "                                   trainable=False)\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    # activation of the outputs\n",
        "    activations = tf.matmul(inputs, self.w)+self.b\n",
        "    # update values (sum and number of calls, will keep their values across calls) \n",
        "    self.sum_activation.assign_add(tf.reduce_sum(activations, axis=0))\n",
        "    self.number_call.assign_add(inputs.shape[0])\n",
        "    return activations, self.sum_activation / tf.cast(self.number_call, tf.float32)\n",
        "\n",
        "dense_layer_mean = MyLayerMean(3,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGc_DJeTLUUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f2e3086-96d6-4368-a207-4534529f53b3"
      },
      "source": [
        "# Test the layer\n",
        "\n",
        "y, activation_means = dense_layer_mean(tf.ones((1, 5)))\n",
        "print(activation_means.numpy())\n",
        "\n",
        "# nothing changes because weights and bias are not updated\n",
        "y, activation_means = dense_layer_mean(tf.ones((1, 5)))\n",
        "print(activation_means.numpy())\n",
        "# Accumulating the mean or variance of the activations can be really useful e.g. for analyzing the propagation of signals in the network. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.03866848  0.03801265 -0.01571291]\n",
            "[-0.03866848  0.03801265 -0.01571291]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74unDX1sLUUM"
      },
      "source": [
        "# Create a Dropout layer as a custom layer\n",
        "\n",
        "class MyDropout(Layer):\n",
        "\n",
        "    def __init__(self, rate):\n",
        "        super(MyDropout, self).__init__()\n",
        "        self.rate = rate\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        # Define forward pass for dropout layer\n",
        "        return tf.nn.dropout(inputs, rate=self.rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeJQLlOaLUUN"
      },
      "source": [
        "#### Implement the custom layers into a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBNDA6QgLUUN"
      },
      "source": [
        "# Build the model using custom layers with the model subclassing API\n",
        "\n",
        "class MyModel(Model):\n",
        "\n",
        "    def __init__(self, units_1, input_dim_1, units_2, units_3):\n",
        "        super(MyModel, self).__init__()\n",
        "        # Define layers\n",
        "        self.layer_1 = MyLayer(units_1, input_dim_1)\n",
        "        self.dropout_1 = MyDropout(0.5)\n",
        "        self.layer_2 = MyLayer(units_2, units_1)\n",
        "        self.dropout_2 = MyDropout(0.5)\n",
        "        self.layer_3 = MyLayer(units_3, units_2)\n",
        "        self.softmax = Softmax()\n",
        "           \n",
        "    def call(self, inputs):\n",
        "        # Define forward pass\n",
        "        x = self.layer_1(inputs)\n",
        "        x = tf.nn.relu(x)  # call activations here, otherwise the layers will be linear\n",
        "        x = self.dropout_1(x)\n",
        "        x = self.layer_2(inputs)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.dropout_2(x)\n",
        "        x = self.layer_3(x)\n",
        "\n",
        "        return self.softmax(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqD_K6AjLUUO"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_3\"></a>\n",
        "## Automatic differentiation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjyrD5WmLUUO"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stiepnYlLUUP"
      },
      "source": [
        "#### Create synthetic data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ_qrLpBLUUQ",
        "outputId": "48471e8e-3d51-4226-d65b-c139b9c950a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "# Create data from a noise contaminated linear model\n",
        "\n",
        "def MakeNoisyData(m, b, n=20):\n",
        "    x = tf.random.uniform(shape=(n,))\n",
        "    noise = tf.random.normal(shape=(len(x),), stddev=0.1)\n",
        "    y = m * x + b + noise\n",
        "    return x, y\n",
        "\n",
        "m=1\n",
        "b=2\n",
        "x_train, y_train = MakeNoisyData(m,b)\n",
        "plt.plot(x_train, y_train, 'b.')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f06f11bf790>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hld6OtzLUUQ"
      },
      "source": [
        "#### Define a linear regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnuJmjOSLUUQ"
      },
      "source": [
        "from tensorflow.keras.layers import Layer"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkccEZvFLUUR",
        "outputId": "06163a0c-6de9-4051-c314-e8dbbd11daa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Build a custom layer for the linear regression model\n",
        "\n",
        "class LinearLayer(Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(LinearLayer, self).__init__()\n",
        "    self.m = self.add_weight(shape=(1,),\n",
        "                             initializer='random_normal')\n",
        "    self.b = self.add_weight(shape=(1,),\n",
        "                             initializer='zeros')\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    return self.m*inputs+self.b\n",
        "\n",
        "linear_regression = LinearLayer()\n",
        "\n",
        "print(linear_regression(x_train))\n",
        "print(linear_regression.weights)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[0.06933199 0.023721   0.07912418 0.03079096 0.04425802 0.01021458\n",
            " 0.04513784 0.0005613  0.06225901 0.06271388 0.00839703 0.03471534\n",
            " 0.02714759 0.06329098 0.08046802 0.05948634 0.03361373 0.01292873\n",
            " 0.05710317 0.05492993], shape=(20,), dtype=float32)\n",
            "[<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.08147576], dtype=float32)>, <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15edBCaOLUUR"
      },
      "source": [
        "#### Define the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k-DUU6gLUUR",
        "outputId": "595753c8-6cfa-41ff-967c-39182a513437",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Define the mean squared error loss function\n",
        "\n",
        "def SquaredError(y_pred, y_true):\n",
        "    return tf.reduce_mean(tf.square(y_pred - y_true)) \n",
        "\n",
        "starting_loss = SquaredError(linear_regression(x_train), y_train)\n",
        "print(\"Starting loss\", starting_loss.numpy())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting loss 6.1552916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78PCHSggLUUS"
      },
      "source": [
        "#### Train and plot the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsyh04g5LUUS",
        "outputId": "d0bfd9e9-f761-4a81-d59d-a089cb470b5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Implement a gradient descent training loop for the linear regression model\n",
        "\n",
        "learning_rate = 0.05\n",
        "steps = 25\n",
        "\n",
        "for i in range(steps):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = linear_regression(x_train)\n",
        "    loss = SquaredError(predictions, y_train)\n",
        "\n",
        "  gradients = tape.gradient(loss, linear_regression.trainable_variables)\n",
        "\n",
        "  linear_regression.m.assign_sub(learning_rate * gradients[0])\n",
        "  linear_regression.b.assign_sub(learning_rate * gradients[0])\n",
        "\n",
        "  print(\"Step %d, Loss %f\" % (i, loss.numpy()))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0, Loss 6.155292\n",
            "Step 1, Loss 5.140264\n",
            "Step 2, Loss 4.295762\n",
            "Step 3, Loss 3.592967\n",
            "Step 4, Loss 3.007949\n",
            "Step 5, Loss 2.520832\n",
            "Step 6, Loss 2.115104\n",
            "Step 7, Loss 1.777052\n",
            "Step 8, Loss 1.495284\n",
            "Step 9, Loss 1.260334\n",
            "Step 10, Loss 1.064337\n",
            "Step 11, Loss 0.900758\n",
            "Step 12, Loss 0.764163\n",
            "Step 13, Loss 0.650038\n",
            "Step 14, Loss 0.554628\n",
            "Step 15, Loss 0.474813\n",
            "Step 16, Loss 0.407995\n",
            "Step 17, Loss 0.352016\n",
            "Step 18, Loss 0.305078\n",
            "Step 19, Loss 0.265688\n",
            "Step 20, Loss 0.232598\n",
            "Step 21, Loss 0.204775\n",
            "Step 22, Loss 0.181353\n",
            "Step 23, Loss 0.161614\n",
            "Step 24, Loss 0.144958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i3p09LQLUUT",
        "outputId": "d79c221d-bf7e-4d5a-9af0-f66987a17923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "# Plot the learned regression model\n",
        "\n",
        "print(\"m:{},  trained m:{}\".format(m,linear_regression.m.numpy()))\n",
        "print(\"b:{},  trained b:{}\".format(b,linear_regression.b.numpy()))\n",
        "\n",
        "plt.plot(x_train, y_train, 'b.')\n",
        "\n",
        "x_linear_regression=np.linspace(min(x_train), max(x_train),50)\n",
        "plt.plot(x_linear_regression, linear_regression.m*x_linear_regression+linear_regression.b, 'r.')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "m:1,  trained m:[1.4811265]\n",
            "b:2,  trained b:[1.3996508]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f06ebf43fd0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G5NMaI6LUUV"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_4\"></a>\n",
        "## Custom training loops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvcprQYVLUUW"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvowknegLUUW"
      },
      "source": [
        "#### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T51Wq6QLUUW"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, Softmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMjThE3yLUUW"
      },
      "source": [
        "# Define the custom layers and model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFVeAi3oLUUX"
      },
      "source": [
        "#### Load the reuters dataset and define the class_names "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crNKTJcWLUUX"
      },
      "source": [
        "# Load the dataset\n",
        "\n",
        "from tensorflow.keras.datasets import reuters\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n",
        "\n",
        "class_names = ['cocoa','grain','veg-oil','earn','acq','wheat','copper','housing','money-supply',\n",
        "   'coffee','sugar','trade','reserves','ship','cotton','carcass','crude','nat-gas',\n",
        "   'cpi','money-fx','interest','gnp','meal-feed','alum','oilseed','gold','tin',\n",
        "   'strategic-metal','livestock','retail','ipi','iron-steel','rubber','heat','jobs',\n",
        "   'lei','bop','zinc','orange','pet-chem','dlr','gas','silver','wpi','hog','lead']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_nw7MGBLUUY"
      },
      "source": [
        "# Print the class of the first sample\n",
        "\n",
        "print(\"Label: {}\".format(class_names[train_labels[0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbCx1JBzLUUY"
      },
      "source": [
        "#### Get the dataset word index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbg5RxiULUUY"
      },
      "source": [
        "# Load the Reuters word index\n",
        "\n",
        "word_to_index = reuters.get_word_index()\n",
        "\n",
        "invert_word_index = dict([(value, key) for (key, value) in word_to_index.items()])\n",
        "text_news = ' '.join([invert_word_index.get(i - 3, '?') for i in train_data[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HCGngsKLUUZ"
      },
      "source": [
        "# Print the first data example sentence\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f1vpdH3LUUa"
      },
      "source": [
        "#### Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQGQvdroLUUa"
      },
      "source": [
        "# Define a function that encodes the data into a 'bag of words' representation\n",
        "\n",
        "def bag_of_words(text_samples, elements=10000):\n",
        "    output = np.zeros((len(text_samples), elements))\n",
        "    for i, word in enumerate(text_samples):\n",
        "        output[i, word] = 1.\n",
        "    return output\n",
        "\n",
        "x_train = bag_of_words(train_data)\n",
        "x_test = bag_of_words(test_data)\n",
        "\n",
        "print(\"Shape of x_train:\", x_train.shape)\n",
        "print(\"Shape of x_test:\", x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbO8liN6LUUa"
      },
      "source": [
        "#### Define the loss function and optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52aLcCVjLUUb"
      },
      "source": [
        "# Define the categorical cross entropy loss and Adam optimizer\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "def loss(model, x, y, wd):\n",
        "    kernel_variables = []\n",
        "    for l in model.layers:\n",
        "        for w in l.weights:\n",
        "            if 'kernel' in w.name:\n",
        "                kernel_variables.append(w)\n",
        "    wd_penalty = wd * tf.reduce_sum([tf.reduce_sum(tf.square(k)) for k in kernel_variables])\n",
        "    y_ = model(x)\n",
        "    return loss_object(y_true=y, y_pred=y_) + wd_penalty\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmTI9s3uLUUb"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl5cnYrULUUb"
      },
      "source": [
        "# Define a function to compute the forward and backward pass\n",
        "\n",
        "def grad(model, inputs, targets, wd):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss(model, inputs, targets, wd)\n",
        "    return loss_value, tape.gradient(loss_value, model.trainable_variables)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xZ6JAHWLUUc"
      },
      "source": [
        "# Implement the training loop\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "    \n",
        "    \n",
        "print(\"Duration :{:.3f}\".format(time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXndbryiLUUc"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t7q07X4LUUd"
      },
      "source": [
        "# Create a Dataset object for the test set\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, test_labels))\n",
        "test_dataset = test_dataset.batch(32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tzyWahXLUUd"
      },
      "source": [
        "# Collect average loss and accuracy\n",
        "\n",
        "epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoGdLJxvLUUd"
      },
      "source": [
        "# Loop over the test set and print scores\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "for x, y in test_dataset:\n",
        "    # Optimize the model\n",
        "    loss_value = loss(model, x, y, weight_decay)    \n",
        "    # Compute current loss\n",
        "    epoch_loss_avg(loss_value)  \n",
        "    # Compare predicted label to actual label\n",
        "    epoch_accuracy(to_categorical(y), model(x))\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(epoch_loss_avg.result().numpy()))\n",
        "print(\"Test accuracy: {:.3%}\".format(epoch_accuracy.result().numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEynBdgSLUUe"
      },
      "source": [
        "#### Plot the learning curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuNWatmCLUUe"
      },
      "source": [
        "# Plot the training loss and accuracy\n",
        "\n",
        "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
        "fig.suptitle('Training Metrics')\n",
        "\n",
        "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
        "axes[0].plot(train_loss_results)\n",
        "\n",
        "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
        "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
        "axes[1].plot(train_accuracy_results)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb01fQDdLUUe"
      },
      "source": [
        "#### Predict from the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MCG0AcSLUUe"
      },
      "source": [
        "# Get the model prediction for an example input\n",
        "\n",
        "predicted_label = np.argmax(model(x_train[np.newaxis,0]),axis=1)[0]\n",
        "print(\"Prediction: {}\".format(class_names[predicted_label]))\n",
        "print(\"     Label: {}\".format(class_names[train_labels[0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxYi1O0eLUUf"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_5\"></a>\n",
        "## tf.function decorator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1qRUx4pLUUf"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import reuters\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7tIhJlmLUUf"
      },
      "source": [
        "#### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKjsmSeFLUUf"
      },
      "source": [
        "# Initialize a new model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chCMaQVHLUUg"
      },
      "source": [
        "#### Redefine the grad function using the @tf.function decorator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SFxLecjLUUg"
      },
      "source": [
        "# Use the @tf.function decorator\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ihrPhcgLUUg"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWAygEjxLUUg"
      },
      "source": [
        "# Re-run the training loop\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2ksbrg0LUUh"
      },
      "source": [
        "#### Print the autograph code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saVdQgvWLUUh"
      },
      "source": [
        "# Use tf.autograph.to_code to see the generated code\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}